{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOXxZp/ayw6NkTEGu1cMy7+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IkxMqV1cY120"},"outputs":[],"source":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"bbsVauRvA0EJ"}},{"cell_type":"markdown","source":["\n","### **1. Supervised Learning**  \n","| **Sub-Type** | **Description** | **Examples** | **Advantages** | **Disadvantages** |\n","|-------------|----------------|-------------|----------------|-------------------|\n","| **Classification** | Predicts discrete labels (categories). | Spam detection, Disease diagnosis | High accuracy with labeled data. | Requires large labeled datasets. |\n","| **Regression** | Predicts continuous values. | House price prediction, Weather forecasting | Works well with linear relationships. | Sensitive to outliers. |\n","| **Algorithms** | Logistic Regression, SVM, Decision Trees, Random Forest. | - | Interpretable models (e.g., Decision Trees). | Overfitting risk (e.g., Deep Neural Networks). |\n","\n","---\n","\n","### **2. Unsupervised Learning**  \n","| **Sub-Type** | **Description** | **Examples** | **Advantages** | **Disadvantages** |\n","|-------------|----------------|-------------|----------------|-------------------|\n","| **Clustering** | Groups similar data points. | Customer segmentation, Image compression | No need for labeled data. | Hard to evaluate performance. |\n","| **Association** | Finds item relationships. | Market basket analysis (e.g., \"Buy X, get Y\") | Useful for recommendation systems. | Computationally expensive for large datasets. |\n","| **Dimensionality Reduction** | Reduces features while preserving info. | PCA for facial recognition | Improves model efficiency. | Loss of interpretability. |\n","| **Algorithms** | K-Means, Apriori, PCA. | - | Works with unlabeled data. | Results may be hard to interpret. |\n","\n","---\n","\n","### **3. Reinforcement Learning (RL)**  \n","| **Sub-Type** | **Description** | **Examples** | **Advantages** | **Disadvantages** |\n","|-------------|----------------|-------------|----------------|-------------------|\n","| **Model-Based RL** | Uses a simulated environment. | Robotics, Self-driving cars (simulation) | Safer training in virtual worlds. | Requires accurate environment modeling. |\n","| **Model-Free RL** | Learns by direct interaction. | Game AI (AlphaGo), Autonomous drones | Adapts to dynamic environments. | High computational cost. |\n","| **Algorithms** | Q-Learning, Deep Q-Networks (DQN). | - | Optimizes long-term rewards. | Slow convergence; needs many trials. |\n","\n","---\n","\n","### **4. Semi-Supervised Learning**  \n","| **Description** | Combines labeled + unlabeled data. |\n","|----------------|-----------------------------------|\n","| **Examples**   | Speech recognition, Medical imaging (few labeled scans). |\n","| **Advantages** | Reduces labeling costs; improves accuracy over unsupervised. |\n","| **Disadvantages** | Complex implementation; performance depends on labeled data quality. |\n","| **Algorithms** | Self-Training, Co-Training. |\n","\n","---\n","\n","### **5. Self-Supervised Learning**  \n","| **Description** | Generates labels from raw data (no human labeling). |\n","|----------------|---------------------------------------------------|\n","| **Examples**   | BERT (NLP), Image colorization. |\n","| **Advantages** | No manual labeling; scalable for large datasets. |\n","| **Disadvantages** | Requires massive data; computationally intensive. |\n","| **Algorithms** | Contrastive Learning (e.g., SimCLR). |\n","\n","---\n","\n","### **6. Transfer Learning**  \n","| **Sub-Type** | **Description** | **Examples** | **Advantages** | **Disadvantages** |\n","|-------------|----------------|-------------|----------------|-------------------|\n","| **Feature Extraction** | Uses pre-trained layers as fixed features. | ResNet for tumor detection | Saves training time/resources. | May not fit new task perfectly. |\n","| **Fine-Tuning** | Adjusts pre-trained model weights. | ChatGPT fine-tuned for customer support | High performance with less data. | Risk of overfitting on small datasets. |\n","\n","---\n","\n","### **7. Deep Learning (Subset of ML)**  \n","| **Sub-Type** | **Description** | **Examples** | **Advantages** | **Disadvantages** |\n","|-------------|----------------|-------------|----------------|-------------------|\n","| **CNN** | Image/Video processing. | Facial recognition, Autonomous vehicles | Excellent for spatial data. | Needs large datasets. |\n","| **RNN/LSTM** | Sequential data (time-series, NLP). | Stock prediction, Speech recognition | Handles temporal dependencies. | Slow training; vanishing gradients. |\n","| **Transformers** | NLP tasks (attention mechanisms). | ChatGPT, BERT | State-of-the-art for text. | Extremely resource-heavy. |\n","\n","---\n","\n","### **Key Takeaways**  \n","- **Best for labeled data?** → **Supervised Learning**.  \n","- **No labels?** → **Unsupervised/Reinforcement Learning**.  \n","- **Limited labels?** → **Semi-Supervised/Self-Supervised**.  \n","- **Reuse models?** → **Transfer Learning**."],"metadata":{"id":"6eyJovpqCZKT"}}]}