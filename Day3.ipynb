{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " **Batch Learning vs Online Learning **\n",
        "\n",
        "| **Aspect**               | **Batch Learning**                          | **Online Learning**                          |\n",
        "|--------------------------|--------------------------------------------|---------------------------------------------|\n",
        "| **Definition**           | Model is trained on the entire dataset at once. | Model is updated incrementally with new data. |\n",
        "| **Data Usage**           | Requires all data upfront.                 | Processes data one instance (or mini-batch) at a time. |\n",
        "| **Computation**          | High memory/CPU (needs full dataset).      | Low memory (processes data sequentially).   |\n",
        "| **Speed**                | Slower (trains on all data).               | Faster (updates in real-time).              |\n",
        "| **Model Updates**        | Retrain from scratch when new data arrives. | Continuously adapts to new data.           |\n",
        "\n",
        "---\n",
        "\n",
        "### **Problems & Challenges**  \n",
        "\n",
        "#### **Batch Learning:**\n",
        "1. **Scalability Issues**  \n",
        "   - Struggles with very large datasets (memory constraints).  \n",
        "2. **Stale Models**  \n",
        "   - Requires retraining from scratch for updates → Not suitable for dynamic data.  \n",
        "3. **Resource-Intensive**  \n",
        "   - Needs heavy computation for each retraining cycle.  \n",
        "\n",
        "#### **Online Learning:**  \n",
        "1. **Catastrophic Forgetting**  \n",
        "   - May \"forget\" old patterns if new data dominates.  \n",
        "2. **Noise Sensitivity**  \n",
        "   - Outliers or noisy data can skew the model quickly.  \n",
        "3. **Hyperparameter Tuning**  \n",
        "   - Harder to debug and optimize (no fixed dataset).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages**  \n",
        "\n",
        "#### **Batch Learning:**  \n",
        "✔ **High Accuracy** – Learns from complete data.  \n",
        "✔ **Stable Training** – Less sensitive to noise (global optimization).  \n",
        "✔ **Reproducibility** – Fixed dataset = consistent results.  \n",
        "\n",
        "#### **Online Learning:**  \n",
        "✔ **Real-Time Adaptation** – Ideal for streaming data (e.g., stock prices).  \n",
        "✔ **Low Resource Use** – No need to store entire datasets.  \n",
        "✔ **Scalability** – Handles infinite data streams efficiently.  \n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use?**  \n",
        "- **Batch Learning:**  \n",
        "  - Small/static datasets (e.g., historical sales analysis).  \n",
        "  - When accuracy > speed (e.g., medical diagnosis).  \n",
        "- **Online Learning:**  \n",
        "  - Dynamic environments (e.g., fraud detection, recommender systems).  \n",
        "  - Limited computational resources (e.g., IoT devices).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Examples**  \n",
        "- **Batch:** Training a CNN on ImageNet.  \n",
        "- **Online:** Updating a spam filter with new emails.  \n"
      ],
      "metadata": {
        "id": "YkF0oZgGZNic"
      }
    }
  ]
}